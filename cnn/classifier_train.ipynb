{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "x = 'x-axis'\n",
    "y = 'y-axis'\n",
    "z = 'z-axis'\n",
    "\n",
    "def read_data(file_name):\n",
    "    data = pd.read_csv(file_name, header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "def normalizeBySD(feature):\n",
    "    mu = np.mean(feature,axis = 0)\n",
    "    sigma = np.std(feature, axis = 0)\n",
    "    return (feature - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = read_data('data/WISDM_ar_v1.1_raw.txt')\n",
    "features.dropna(axis=0, how='any', inplace= True)\n",
    "features[z] = features[z].str.replace(';','')\n",
    "features[z] = features[z].astype(np.float64)\n",
    "features.drop_duplicates(column_names, keep= 'first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[x] = normalizeBySD(features[x])\n",
    "features[y] = normalizeBySD(features[y])\n",
    "features[z] = normalizeBySD(features[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activity = ['Walking', 'Jogging',  'Sitting', 'Standing', 'Downstairs', 'Upstairs']\n",
    "step = 15\n",
    "num_rows = 90\n",
    "\n",
    "def segmentDataset(featureList):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(featureList) - num_rows, step):\n",
    "        xs = featureList[x].values[i: i + num_rows]\n",
    "        ys = featureList[y].values[i: i + num_rows]\n",
    "        zs = featureList[z].values[i: i + num_rows]\n",
    "        try:\n",
    "            label = stats.mode(featureList[\"activity\"][i: i + num_rows])[0][0]\n",
    "            if label in activity:\n",
    "                labels = np.append(labels,label)\n",
    "                segments.append([xs, ys, zs])\n",
    "            else:\n",
    "                print(i, i+num_rows)\n",
    "        except TypeError:\n",
    "            print(i, i+num_rows)\n",
    "            \n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "segments, new_label = segmentDataset(features)\n",
    "labels = np.asarray(pd.get_dummies(new_label), dtype = np.int8)\n",
    "segments = np.array(segments)\n",
    "reshaped_segments = segments.reshape(len(segments),1, 90, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(reshaped_segments, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 6\n",
    "batch_size = 10\n",
    "input_name = 'input'\n",
    "output_name = 'output'\n",
    "\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_channels = 3\n",
    "\n",
    "kernel_size_pool = 20\n",
    "stride_pool = 2\n",
    "\n",
    "kernel_size2 = 6\n",
    "depth2 = 6\n",
    "num_channel2 = depth * num_channels\n",
    "\n",
    "num_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_weights(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def initialize_biases(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels], name=input_name)\n",
    "\n",
    "conv1_filters = initialize_weights([1, kernel_size, num_channels, depth])\n",
    "conv1_biases = initialize_biases([depth * num_channels])\n",
    "c = tf.nn.relu(tf.add(tf.nn.depthwise_conv2d(X, conv1_filters, [1, 1, 1, 1], padding='VALID'), conv1_biases))\n",
    "\n",
    "p = tf.nn.max_pool(c, ksize=[1, 1, kernel_size_pool, 1], strides=[1, 1, stride_pool, 1], padding='VALID')\n",
    "\n",
    "conv2_filters = initialize_weights([1, kernel_size2, num_channel2, depth2])\n",
    "conv2_biases = initialize_biases([depth2 * num_channel2])\n",
    "c = tf.nn.relu(tf.add(tf.nn.depthwise_conv2d(p, conv2_filters, [1, 1, 1, 1], padding='VALID'), conv2_biases))\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "flat_weights = initialize_weights([shape[1] * shape[2] * depth * num_channels * (depth2), num_hidden])\n",
    "flat_biases = initialize_biases([num_hidden])\n",
    "flat = tf.nn.tanh(tf.add(tf.matmul(c_flat, flat_weights),flat_biases))\n",
    "\n",
    "out_weights = initialize_weights([num_hidden, num_labels])\n",
    "out_biases = initialize_biases([num_labels])\n",
    "pred_Y = tf.matmul(flat, out_weights) + out_biases\n",
    "y_ = tf.nn.softmax(pred_Y, name=output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "training_epochs = 2 #50\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "right_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(right_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "history = dict(train_loss=[], train_acc=[], test_loss=[], test_acc=[])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            session.run(optimizer,feed_dict={X: batch_x, Y : batch_y})\n",
    "            \n",
    "        _, loss_tr, accuracy_tr = session.run([y_, loss, accuracy],feed_dict={X: train_x, Y : train_y})\n",
    "        _, loss_te, accuracy_te = session.run([y_, loss, accuracy], feed_dict={X: test_x, Y: test_y})\n",
    "        history['train_loss'].append(loss_tr)\n",
    "        history['train_acc'].append(accuracy_tr)\n",
    "        history['test_loss'].append(loss_te)\n",
    "        history['test_acc'].append(accuracy_te)\n",
    "        \n",
    "        \n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",loss_tr,\" Training Accuracy: \", accuracy_tr)\n",
    "        \n",
    "    predictions, acc_final, loss_final = session.run([y_, accuracy, loss], feed_dict={X: test_x, Y: test_y})\n",
    "    print (\"Final loss:\", loss_final, \"Final Accuracy:\", acc_final)\n",
    "    pickle.dump(predictions, open(\"predictions.p\", \"wb\"))\n",
    "    pickle.dump(history, open(\"history.p\", \"wb\"))\n",
    "    tf.train.write_graph(session.graph_def, '.', './checkpoint/har.pbtxt')  \n",
    "    saver.save(session, save_path = \"./checkpoint/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = pickle.load(open(\"history.p\", \"rb\"))\n",
    "predictions = pickle.load(open(\"predictions.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "fig.suptitle(\"Training session's progress over iterations\", fontsize=20)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(np.array(history['train_loss']), \"r--\", label=\"Train loss\")\n",
    "ax2.plot(np.array(history['test_loss']), \"r-\", label=\"Test loss\")\n",
    "\n",
    "ax1.plot(np.array(history['train_acc']), \"g--\", label=\"Train accuracy\")\n",
    "ax1.plot(np.array(history['test_acc']), \"g-\", label=\"Test accuracy\")\n",
    "\n",
    "ax1.legend(shadow=True, loc='best', fontsize=20)\n",
    "ax2.legend(shadow=True, loc='right', fontsize=20)\n",
    "\n",
    "ax2.set_ylabel('Loss values', fontsize=20)\n",
    "ax1.set_xlabel('Training Epoch', fontsize=20)\n",
    "ax1.set_ylabel('Accuracy values', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LABELS = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n",
    "max_test = np.argmax(test_y, axis=1)\n",
    "max_predictions = np.argmax(predictions, axis=1)\n",
    "confusion_matrix = metrics.confusion_matrix(max_test, max_predictions)\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", cmap=\"YlGnBu\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'har'\n",
    "\n",
    "input_graph_path = 'checkpoint/' + MODEL_NAME+'.pbtxt'\n",
    "checkpoint_path = './checkpoint/' +MODEL_NAME+'.ckpt'\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "output_frozen_graph_name = 'har_classifier.pb'\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph_path, input_saver=\"\",\n",
    "                          input_binary=False, input_checkpoint=checkpoint_path, \n",
    "                          output_node_names=output_name, restore_op_name=\"save/restore_all\",\n",
    "                          filename_tensor_name=\"save/Const:0\", \n",
    "                          output_graph=output_frozen_graph_name, clear_devices=True, initializer_nodes=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
